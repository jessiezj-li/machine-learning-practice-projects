{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github Repository Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "We first import all the libraries needed for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#! pip install implicit\n",
    "#! pip install \"scipy>=1.0\"\n",
    "#! pip install scikit-surprise\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark= pyspark.sql.SQLContext(sc)\n",
    "\n",
    "import implicit\n",
    "from scipy import sparse\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Pre-processing\n",
    "We retrive the github data on \"watchers\" (users can watch repositories) from public dataset \"ghtorrent-bq\" on Google BigQuery. It is not necessary to use all the data to generate recommendations, also some data may not even add much information, for example users who rarely watch repositories or user who watch almost \"every\" (a large number of) repositories. Therefore, we first filter data in BigQuery to retrieve the list of most popular repositories (top 500 being watched by users) and the users who watch these repositories (10,000 random users who has wathched 20 - 100 of these repositories).\n",
    "\n",
    "The SQL Query for generating the raw data is shown as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT repo_id, user_id FROM [ghtorrent-bq:ght.watchers] \n",
    "WHERE user_id IN \n",
    "\n",
    "(SELECT user_id FROM\n",
    "\n",
    "(SELECT user_id, COUNT(*) AS num_watching_repo\n",
    "\n",
    "FROM [ghtorrent-bq:ght.watchers] \n",
    "\n",
    "GROUP BY user_id\n",
    "\n",
    "HAVING num_watching_repo > 10 AND num_watching_repo < 100\n",
    "\n",
    "LIMIT 10000))\n",
    "\n",
    "AND repo_id IN\n",
    "\n",
    "(SELECT repo_id FROM\n",
    "\n",
    "(SELECT repo_id, COUNT(*) AS num_watcher\n",
    "\n",
    "FROM [ghtorrent-bq:ght.watchers] \n",
    "\n",
    "GROUP BY repo_id\n",
    "\n",
    "ORDER BY num_watcher DESC\n",
    "\n",
    "LIMIT 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>repo_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>634204</td>\n",
       "      <td>20078281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4605019</td>\n",
       "      <td>20078281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5107395</td>\n",
       "      <td>20078281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8748144</td>\n",
       "      <td>20078281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>341184</td>\n",
       "      <td>22042207</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id   repo_id  rating\n",
       "0   634204  20078281       1\n",
       "1  4605019  20078281       1\n",
       "2  5107395  20078281       1\n",
       "3  8748144  20078281       1\n",
       "4   341184  22042207       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data from csv file into a dataframe and add implicit rating column\n",
    "raw_data = pd.read_csv(\"data/top500repo.csv\")\n",
    "ratings = [1] * raw_data.shape[0]\n",
    "raw_data['rating'] = ratings\n",
    "#reorder the dataframe\n",
    "columns_order = ['user_id', 'repo_id', 'rating']\n",
    "raw_data = raw_data.reindex(columns=columns_order)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the raw data only contains the repositories that these users are watching, i.e. the rating numbers are all 1, we need to include the repositories that the users are not watching and add rating number (in this case we use 0) for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4354000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first create a pivot table and fill all the NaNs as 0\n",
    "data_matrix = raw_data.pivot(index='user_id', columns = 'repo_id', values = 'rating')\n",
    "data_matrix = data_matrix.fillna(0)\n",
    "#then convert the pivot table back to dataframe, \n",
    "#so that we get the full combinations of users and repos\n",
    "use_data = data_matrix.stack().reset_index(name=\"rating\")\n",
    "use_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 SVD in scikit-surprise package \n",
    "The first method we use to generate recommendations is the SVD method available in scikit-surprise package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the libraries needed for SVD\n",
    "from surprise import Reader\n",
    "from surprise import SVDpp\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import KFold\n",
    "#since the default rating scale range is 0-5, we need to adjust the range\n",
    "reader = Reader(rating_scale = (0.0,1.0))\n",
    "#load data from dataframe\n",
    "data = Dataset.load_from_df(use_data, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data into training and testing set\n",
    "trainset, testset = train_test_split(data, test_size=.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.1055\n",
      "RMSE: 0.1053\n",
      "RMSE: 0.1052\n"
     ]
    }
   ],
   "source": [
    "# define a cross-validation iterator\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "algo = SVD()\n",
    "\n",
    "for trainset, testset in kf.split(data):\n",
    "\n",
    "    # train and test algorithm.\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune algorithm parameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
    "              'reg_all': [0.4, 0.6]}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SVD with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can now use the algorithm that yields the best rmse:\n",
    "algo = gs.best_estimator['rmse']\n",
    "algo.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 ALS in Spark\n",
    "The second method we use is the ALS algorithm available in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries needed\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the pandas dataframe \"use_data\" into spark dataframe\n",
    "ratings = spark.createDataFrame(use_data)\n",
    "#Split data into training and testing sets\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the training data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "als = ALS(maxIter=5, regParam=0.01, implicitPrefs=True, userCol=\"user_id\", itemCol=\"repo_id\", ratingCol=\"rating\")\n",
    "model = als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.10617955065788996\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 ALS in implicit package\n",
    "The third method we use is the ALS algorithm available in implicit package. The implicit package does not inlude any evaluation method for the model, however it can generate recommendations for specific users and return related repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert the data matrix to repo-user csr matrix\n",
    "csr_repo_user = sparse.csr_matrix(data_matrix).T.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15.0/15 [00:40<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "#fit model\n",
    "implicit_model = implicit.als.AlternatingLeastSquares(factors=50)\n",
    "implicit_model.fit(csr_repo_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18, 0.42213416),\n",
       " (23, 0.038269203),\n",
       " (17, 0.033236597),\n",
       " (77, 0.031356655),\n",
       " (211, 0.027273616),\n",
       " (221, 0.025331859),\n",
       " (8, 0.025092404),\n",
       " (161, 0.021297604),\n",
       " (159, 0.021291038),\n",
       " (344, 0.02116368)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommend repos for a user\n",
    "user_repos = csr_repo_user.T.tocsr()\n",
    "recommendations = implicit_model.recommend(227, user_repos)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 0.6067302),\n",
       " (254, 0.28099835),\n",
       " (174, 0.2669173),\n",
       " (347, 0.25447154),\n",
       " (223, 0.25257245),\n",
       " (156, 0.2522952),\n",
       " (430, 0.21773125),\n",
       " (198, 0.21363513),\n",
       " (333, 0.20966932),\n",
       " (25, 0.19306782)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find related repos\n",
    "related = implicit_model.similar_items(15)\n",
    "related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
